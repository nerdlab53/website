<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>>> hello</title>
    <link rel="stylesheet" type="text/css" href="style.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
    <link rel="icon" type="image/png" href="maneki-neko.png">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: 'SF Mono', monospace;
        }
        .math-table {
            border-collapse: collapse;
            margin: 15px 0;
            width: 100%;
            max-width: 600px;
        }
        .math-table th, .math-table td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        .math-table th {
            background-color: #f5f5f5;
        }
        .example-section {
            margin: 20px 0;
            padding: 15px;
            background-color: #f9f9f9;
            border-radius: 5px;
        }
        .math-formula {
            margin: 15px 0;
            padding: 10px;
            background-color: #fff;
            border-radius: 3px;
        }
        .example-section {
        margin: 20px 0;
        padding: 15px;
        background-color: #1a1a1a;  /* Dark background */
        border-radius: 5px;
        color: #ffffff;  /* Light text for dark background */
    }
    .math-table {
        border-collapse: collapse;
        margin: 15px 0;
        width: 100%;
        max-width: 600px;
        background-color: #2d2d2d;  /* Slightly lighter than background */
    }
    .math-table th, .math-table td {
        border: 1px solid #444;  /* Darker border */
        padding: 8px;
        text-align: left;
    }
    .math-table th {
        background-color: #363636;  /* Slightly lighter than table background */
        color: #ffffff;
    }
    .math-formula {
        margin: 15px 0;
        padding: 10px;
        background-color: #2d2d2d;  /* Slightly lighter than background */
        border-radius: 3px;
        color: #ffffff;
    }
    /* Add styles for lists and links in dark mode */
    .example-section ul, .example-section ol {
        color: #ffffff;
    }
    .example-section a {
        color: #66b3ff;  /* Light blue for links */
    }
    .explanation-section {
        background-color: #1a1a1a;
        color: #ffffff;
        padding: 2rem;
        border-radius: 8px;
        margin: 2rem 0;
    }

    .question {
        background-color: #2d2d2d;
        padding: 1rem;
        border-left: 4px solid #66b3ff;
        margin: 1rem 0;
        font-style: italic;
    }

    .concept-block {
        margin: 2rem 0;
    }

    .circuit-explanation {
        margin: 1.5rem 0;
        padding: 1rem;
        background-color: #2d2d2d;
        border-radius: 6px;
    }

    .example-box {
        background-color: #363636;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 4px;
    }

    .example-box h6 {
        color: #66b3ff;
        margin-top: 0;
    }

    .example-box ul {
        margin: 0.5rem 0;
        padding-left: 1.5rem;
    }

    h4, h5 {
        color: #66b3ff;
        margin-bottom: 1rem;
    }

    strong {
        color: #66b3ff;
    }

    .math-formula {
        background-color: #2d2d2d;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 4px;
    }
    /* Dark mode styles for math formulas and examples */
    .math-formula {
        background-color: #2d2d2d;
        color: #ffffff;
        padding: 1.5rem;
        margin: 1rem 0;
        border-radius: 6px;
        border: 1px solid #444;
    }

    .example-box {
        background-color: #2d2d2d;
        color: #ffffff;
        padding: 1.5rem;
        margin: 1rem 0;
        border-radius: 6px;
        border: 1px solid #444;
    }

    /* MathJax specific dark mode adjustments */
    .MathJax {
        color: #ffffff !important;
    }

    /* Matrix styling in dark mode */
    .math-formula .MathJax_CHTML {
        background-color: transparent !important;
    }

    /* Headings in dark sections */
    .explanation-section h4,
    .explanation-section h5 {
        color: #66b3ff;
        margin-top: 1.5rem;
        margin-bottom: 1rem;
    }

    /* Lists in dark sections */
    .explanation-section ul,
    .explanation-section ol {
        color: #ffffff;
        padding-left: 1.5rem;
    }

    .explanation-section li {
        margin: 0.5rem 0;
    }

    /* Strong text emphasis */
    .explanation-section strong {
        color: #66b3ff;
        font-weight: 600;
    }

    /* Paragraph text */
    .explanation-section p {
        color: #ffffff;
        line-height: 1.6;
        margin: 1rem 0;
    }

    /* Borders and separators */
    .explanation-section > div {
        border-bottom: 1px solid #444;
        padding-bottom: 1.5rem;
        margin-bottom: 1.5rem;
    }

    .explanation-section > div:last-child {
        border-bottom: none;
    }

    /* Code blocks if any */
    .explanation-section code {
        background-color: #363636;
        color: #66b3ff;
        padding: 0.2rem 0.4rem;
        border-radius: 3px;
    }
    </style>
</head>
<body>
    <button class="toggle-btn" id="themeToggle"><i class="fas fa-moon"></i></button>
    <img src="_ (3).jpeg" alt="P3">
    <br>
    <h2>a dummys questions on the road to mech interp proficiency</h2>
    <hr>
    <h3><b>1. what is meant by the vector space of neuron activations in circuits?</b></h3>
    <p><small>>the vector space of neuron activations refers to the mathematical representation of the activations of neurons in a neural network layer as vectors. <br>
        >sub points: <br>
        1. each neuron in the layer can be thought of as a basis direction in the vector space of the activations for that layer. 
        Activation → Scalar → Set(Activations) forms → Vector. <br>
        2. any direction in the activation space can be represented as a linear combination of the basis directions (individual neurons). So the vector space spans all possible activation patterns of the neurons. <br>
        3. directions often correspond to meaningful features that respond to properties of the input. (e.g curves, textures).</p>
    <hr>
    <h3><b>2. how do the polysemantic neurons affect the vector space of neuron activations?</b></h3>
    <p>>polysemantic neurons influence the vector space of neuron activations by representing multiple features (which can even be unrelated features) within a single neuron. <br>
        it complicates the interpretation of neural networks as well as affect how they are organized and represented in the activation space. <br>
        1.they do not correspond to unique features; instead they activate for a mixture of unrelated features. making the activation space a collection which can have distinct directions as well as rather a more complex structure in which multiple features can overlap within the same neuron <br>
        2.polysemanticity is more prevalent for less important features, which may be represented by fewer dimensions in the embedding space. this leads to a situation where the vector space is not fully utilized for distinct features, resulting in a less interpretable structure. <br> 
        3.in datasets with correlated features, neural networks may opt to represent these features with a single neuron rather than separate neurons. this will complicate the interpretation of activation space.</p>
    <hr>
    <h3><b>3. explain bigrams and skip-trigrams</b></h3>
    <p>
        <div class="example-section">
            <h3>Example Corpus</h3>
            <p>Consider the following three sentences:</p>
            <ol>
                <li>"I love programming."</li>
                <li>"Programming is fun."</li>
                <li>"I love fun."</li>
            </ol>
        
            <h3>Step 1: Extract Bigrams</h3>
            <p>From the corpus, we can extract the bigrams as follows:</p>
            <ul>
                <li>From "I love programming":
                    <ul>
                        <li>(I, love)</li>
                        <li>(love, programming)</li>
                    </ul>
                </li>
                <li>From "Programming is fun":
                    <ul>
                        <li>(Programming, is)</li>
                        <li>(is, fun)</li>
                    </ul>
                </li>
                <li>From "I love fun":
                    <ul>
                        <li>(I, love)</li>
                        <li>(love, fun)</li>
                    </ul>
                </li>
            </ul>
        
            <table class="math-table">
                <tr>
                    <th>Bigram</th>
                    <th>Count</th>
                </tr>
                <tr>
                    <td>(I, love)</td>
                    <td>2</td>
                </tr>
                <tr>
                    <td>(love, programming)</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>(Programming, is)</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>(is, fun)</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>(love, fun)</td>
                    <td>1</td>
                </tr>
            </table>
        
            <h3>Step 2: Calculate Bigram Probabilities</h3>
            <p>Using Maximum Likelihood Estimation (MLE), we calculate the probabilities:</p>
            <div class="math-formula">
                \[P(love | I) = \frac{C(I, love)}{C(I)} = \frac{2}{2} = 1.0\]
                \[P(programming | love) = \frac{C(love, programming)}{C(love)} = \frac{1}{3} \approx 0.33\]
                \[P(is | Programming) = \frac{C(Programming, is)}{C(Programming)} = \frac{1}{1} = 1.0\]
                \[P(fun | is) = \frac{C(is, fun)}{C(is)} = \frac{1}{1} = 1.0\]
                \[P(fun | love) = \frac{C(love, fun)}{C(love)} = \frac{1}{3} \approx 0.33\]
            </div>
        
            <h3>Step 3: Extract Skip Trigrams</h3>
            <!-- Similar formatting for the skip trigrams section -->
            <table class="math-table">
                <tr>
                    <th>Skip Trigram</th>
                    <th>Count</th>
                </tr>
                <tr>
                    <td>(I, programming)</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>(Programming, fun)</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>(I, fun)</td>
                    <td>1</td>
                </tr>
            </table>
        
            <h3>Step 4: Calculate Skip Trigram Probabilities</h3>
            <div class="math-formula">
                \[P(programming | I) = \frac{C(I, programming)}{C(I)} = \frac{1}{2} = 0.5\]
                \[P(fun | I) = \frac{C(I, fun)}{C(I)} = \frac{1}{2} = 0.5\]
                \[P(fun | Programming) = \frac{C(Programming, fun)}{C(Programming)} = \frac{1}{1} = 1.0\]
            </div>
        </div>
    </p>
    <hr>
    <script src="themeToggle.js"></script>
    <div class="explanation-section">
        <h3>4. Explain the claims using examples:</h3>
        <blockquote class="question">
            "Attention heads can be understood as having two largely independent computations: a QK ("query-key") circuit which computes the attention pattern, and an OV ("output-value") circuit which computes how each token affects the output if attended to."
        </blockquote>
        <img src="./mech interp images/attention calculation.png" alt="">
        <div class="content">
            <p class="intro">
                The claims regarding attention heads in transformers can be understood through the concepts of QK (query-key) circuits and OV (output-value) circuits. Each of these circuits plays a distinct role in the attention mechanism, which is fundamental to how transformers process information.
            </p>
    
            <section class="concept-block">
                <h4>Understanding QK and OV Circuits</h4>
                
                <div class="circuit-explanation">
                    <h5>1. QK Circuit</h5>
                    <p>The QK circuit is responsible for computing the <strong>attention pattern</strong>. It determines how much focus one token should place on another token in the input sequence. This is done by calculating the dot product between query vectors (Q) and key vectors (K). The resulting scores indicate the strength of attention that should be applied to each token.</p>
                    
                    <div class="example-box">
                        <h6>Example:</h6>
                        <ul>
                            <li>Consider a sentence: "The cat sat on the mat."</li>
                            <li>Suppose we have a query vector for "cat" and key vectors for all tokens.</li>
                            <li>The QK circuit computes the attention scores between "cat" and all other tokens, determining which tokens "cat" should attend to based on their relevance.</li>
                        </ul>
                    </div>
                </div>
    
                <div class="circuit-explanation">
                    <h5>2. OV Circuit</h5>
                    <p>The OV circuit computes how each token's value vector (V) contributes to the final output if attended to. After determining which tokens are attended to via the QK circuit, the OV circuit aggregates these contributions to produce the final output vector.</p>
                    
                    <div class="example-box">
                        <h6>Example:</h6>
                        <ul>
                            <li>Continuing with our previous example, if "cat" attends to "the" and "sat," the OV circuit will combine their value vectors based on the attention scores computed by the QK circuit.</li>
                            <li>This combination yields a new representation that reflects the context of "cat" within the sentence.</li>
                        </ul>
                    </div>
                </div>
            </section>
        </div>
    </div>
    <div>
        <h3>5. what is meant by the following claim?</h3>
        <blockquote class="question">"One basic consequence is that the residual stream doesn't have a "privileged basis"; we could rotate it by rotating all the matrices interacting with it, without changing model behavior."</blockquote>
        <div class="content">
            <div class="intro">
                <div class="explanation-section">
                    <h4>Understanding the Residual Stream</h4>
                    <p>The claim that "the residual stream doesn't have a 'privileged basis'; we could rotate it by rotating all the matrices interacting with it, without changing model behavior" refers to the flexibility and independence of the residual stream in transformer architectures. Here’s a breakdown of what this means, along with examples for clarity.</p>
                    <p>The residual stream in transformer models acts as a shared memory between layers, where:</p>
                    
                    <div class="math-formula">
                        <h5>Reading from the Residual Stream:</h5>
                        \[ Output = W \cdot ResidualStream \]
                        <p>where W is a weight matrix</p>
                        
                        <h5>Writing to the Residual Stream:</h5>
                        \[ NewResidualStream = PreviousResidualStream + Output \]
                    </div>

                    <h4>The "No Privileged Basis" Concept</h4>
                    <p>The claim means that the residual stream's coordinate system is arbitrary and can be transformed without affecting the model's behavior.</p>
                    
                    <div class="example-box">
                        <h5>Key Properties:</h5>
                        <ul>
                            <li>Arbitrary Basis: Individual coordinates have no inherent meaning</li>
                            <li>Rotational Freedom: Any full-rank linear transformation can be applied</li>
                        </ul>
                    </div>

                    <div class="math-formula">
                        <h5>Example: 2D Rotation</h5>
                        <p>For a residual stream vector \( r=[x,y] \) and rotation matrix R:</p>
                        \[ R = \begin{bmatrix} 
                        \cos(\theta) & -\sin(\theta) \\
                        \sin(\theta) & \cos(\theta)
                        \end{bmatrix} \]
                        
                        <p>The transformation:</p>
                        \[ r' = R \cdot r \]
                        \[ W' = R^{-1}W \]
                    </div>

                    <h4>Implications</h4>
                    <div class="example-box">
                        <ul>
                            <li><strong>Model Consistency:</strong> Behavior remains unchanged with uniform transformations</li>
                            <li><strong>Interpretability:</strong> Dimension importance comes from training, not inherent properties</li>
                            <li><strong>Feature Representation:</strong> Allows flexible exploration of internal representations</li>
                        </ul>
                    </div>
                </div>

            </div>
        </div>

    </div>
    <nav style="text-align: center; padding: 20px 0;">
        <a href="./index.html">home</a> |
        <a href="./cv.html">cv</a> |
        <a href="./posts.html">blogs</a>
    </nav>
</body>
</html>
